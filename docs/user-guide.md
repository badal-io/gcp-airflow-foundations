## Introduction
[![PyPI version](https://badge.fury.io/py/gcp-airflow-foundations.svg)](https://badge.fury.io/py/gcp-airflow-foundations)  

Airflow is an awesome open source orchestration framework that is the go-to for building data ingestion pipelines on GCP (using Composer - a hosted AIrflow service). However, most companies using it face the same set of problems 
Learning curve: Airflow requires python knowledge and has some gotchas that take time to learn. Further, writing Python DAGs for every single table that needs to get ingested becomes cumbersome. Most companies end up building utilities for creating DAGs out of configuration files to simplify DAG creation and to allow non-developers to configure ingestion
Datalake and data pipelines design best practices: Airflow only provides the building blocks, users are still required to understand and implement the nuances of building a proper ingestion pipelines for the data lake/data warehouse platform they are using 
Core reusability and best practice enforcement across the enterprise: Usually each team maintains its own Airflow source code and deployment

The _gcp-airflow-foundations_ module provides an opinionated yet flexible framework for building an ingestion pipeline into a data warehouse in BigQuery that supports the following features:

Zero-code, config file based ingestion - anybody can start ingesting from the growing number of sources by just providing a simple configuration file. Zero python or Airflow knowledge is required 
Modular and extendable - The core of the framework is a lightweight library. Ingestion sources are added as plugins. Adding a new source can be done by extending the provided base classes
Opinionated automatic creation of  ODS (Operational Data Store ) and HDS (Historical Data Store) in BigQuery while enforcing best practices such as schema migration, data quality validation, idempotency, partitioning, etc.
Dataflow job support for ingesting large datasets from SQL sources and deploying jobs into a specific network or shared VPC
Support of advanced Airflow features for job prioritization such as slots and priorities 
Integration with GCP data services such as DLP and Data Catalog [work in progress] 
Well tested - We maintain a rich suite of both unit and integration tests

## Installing from PyPI
```bash
pip install 'gcp-airflow-foundations'
```

## Usage
###Configuring the Data Ingestion Pipeline
After installing _gcp-airflow-foundations_ in your Airflow environment, the user needs to first create a YAML configuration file.
The package reads the YAML configuration files from a common directory. The user should create a new Airflow variable ```CONFIG_FILE_LOCATION``` 
to store the absolute path to that directory. A configuration file consists of two main keys: 1) source and 2) tables, 
that are each used to declare the configuration parameters for a single source and its tables respectively. For example:
```yaml
---
source:
  name: "test_customer_data"
  source_type: GCS
  ingest_schedule: "@daily"
  start_date: "2021-08-30"
  acceptable_delay_minutes: 5
  gcp_project: gcp_project
  dataset_data_name: dataset_data_name
  connection: google_cloud_default
  notification_emails:
    - ''
  owner: owner
  version: 1
  landing_zone_options:
    landing_zone_dataset: landing_zone_dataset
tables:
  - table_name: test_customer_data
    surrogate_keys: ['customer_id','key']
    ods_config:
      ingestion_type: INCREMENTAL
      ods_metadata:
        hash_column_name: af_metadata_row_hash
        primary_key_hash_column_name: af_metadata_primary_key_hash
        ingestion_time_column_name: af_metadata_inserted_at
        update_time_column_name: af_metadata_updated_at
    hds_config:
      hds_table_type: SCD2
      hds_table_time_partitioning: DAY
      hds_metadata:
        eff_start_time_column_name: af_metadata_created_at
        eff_end_time_column_name: af_metadata_expired_at
        hash_column_name: af_metadata_row_hash
    column_mapping: 
      customer_id: customerID
      key: key_id
      city: city_name
```
### Parsing and Validating the Configuration Parameters
The YAML files are loaded as dictionaries and then converted to data classes using the open-source [dacite](https://github.com/konradhalas/dacite) Python library. 
Each of the data classes used have their own validators to ensure that the parameters selected by the user are valid. 
For instance, an error will be raised if the ingestion schedule and the partition time of a snapshot HDS table are not compatible with each other. 

### Generating DAGs from the Configuration Files
The package supports the dynamic generation of DAGs from the user-provided configuration files with minimal code. 
All that is required is that each DAG object is added to Python’s current global symbol table inside the DAG folder of your Airflow environment. 
To do so, the DAGs are generated by calling the DagParser.parse_dags() method in a Python script and added to the globals dictionary:
```python
from gcp_airflow_foundations.parse_dags import DagParser

parser = DagParser() 

# Can overwrite the "CONFIG_FILE_LOCATION" variable here by setting the parser.conf_location attribute to the desired path

parsed_dags = parser.parse_dags()

if parsed_dags:
    globals().update(parsed_dags)
```
### Creating an Operational Data Store in BigQuery
An Operational Data Store (ODS) is a database that provides a snapshot of the latest data for operational reporting. 
As newer records become available, the ODS continuously overwrites older data with either full or incremental data ingestions. With full ingestions, 
the entire ODS is replaced with the updated data, whereas with incremental ingestions only the difference between the target and source data is loaded.   

To implement an ODS with the _gcp-airflow-foundations_ library, the user must provide ODS configuration parameters in the YAML file, 
including ingestion type (full or incremental), and has the option to overwrite the default metadata column names. 
The metadata columns are part of the target ODS table and are used to store information pertaining to the data ingestion for each record, 
including the original ingestion time, the update time, the hash of the entire column, as well as the hash of the primary keys.

### Creating a Historical Data Store in BigQuery
To implement a Historical Data Store (HDS) in BigQuery using _gcp-airflow-foundations_ the user can choose between a Slowly Changing Dimension Type 2 (SCD2) and a snapshot dimension. 
In SCD2, a new row is inserted for each change to an existing record in the corresponding target table, as well as for entirely new records. 
Each record row has metadata timestamp columns that indicate the time of insertion, update, and expiration. With dimension snapshots, 
a new partition is appended to the target table at each ETL schedule. 
Therefore, the target table comprises a collection of dimension snapshots where each partition contains the full dimension at a point in time. 
Even though the SCD2 approach is more computationally efficient, it is also more difficult to maintain and reproduce.  
The drawbacks of SCD2 can be mitigated by snapshot dimensions as they are immutable and do not require complex transformations during ingestion.
Snapshotting, however, has its own tradeoffs. As with other types of denormalization, 
snapshot dimensions can have a large impact on storage space as the target tables grow taller by each inserted partition.  

Similarly to ODS, the user must specify the HDS configuration parameters in the YAML file, including dimension type (SCD2 or snapshot) 
and optionally overwrite the default metadata column names.

### Column Mapping and Schema Migration
#### Column Mapping
Both ODS and HDS ingestions support column mapping and schema migration. When the columns of a source table should be renamed in the target table, 
the user can provide a dictionary in the YAML file whose keys indicate the source column name and the values indicate the corresponding target column name:  
```yaml
    column_mapping: 
      customer_id: customerID
      key: key_id
      city: city_name
```
 Note that columns whose name is unchanged do not have to be included in the dictionary.
 
 #### Schema Migration
 _gcp-airflow-foundations_ supports most schema modifications that are currently supported by BigQuery, including:  

- Changing a column's data type using the current conversion rules in Standard SQL
- Relaxing a column's mode from REQUIRED to NULLABLE  

Note that a column’s mode is relaxed by default when that column is removed in the provided schema. 
Prior to every ODS or HDS ingestion, the table’s current schema is queried and compared against the schema object declared in the YAML file, 
such that any changes made to the latter are migrated to the target table. Before attempting to execute the schema migration, 
the framework will validate any detected data type changes to confirm that they are supported by BigQuery. 
Furthermore, a table is also created in BigQuery to log all the schema migration operations for auditing purposes. 
The audit table stores information on the table and dataset name, 
the timestamp of the schema migration, the columns affected, and the type of schema change that was performed.

## Configuration File Parameters Overview
### Source Configuration Parameters

|Name                    |Data Type        |Description                                                                              |
|------------------------|-----------------|-----------------------------------------------------------------------------------------|
|name                    |String           |Name of source                                                                           |
|source_type             |SourceType       |Type of source                                                                           |
|ingest_schedule         |String           |Data ingestion schedule. Currently only supporting @hourly, @daily, @weekly, and @monthly|
|gcp_project             |String           |Google Cloud Platform project ID                                                         |
|dataset_data_name       |String           |Target dataset name                                                                      |
|connection              |String           |Airflow Google Cloud Platform connection ID                                              |
|extra_options           |Dictionary       |GCP bucket and objects for source data if loading from GCS                               |
|landing_zone_options    |LandingZoneConfig|Staging dataset name                                                                     |
|acceptable_delay_minutes|String           |Delay limit in minutes                                                                   |
|notification_emails     |List[String]     |Email address for notification emails                                                    |
|owner                   |String           |Airflow user owning the DAG                                                              |
|start_date              |String           |Start date for DAG                                                                       |
|start_date_tz           |String           |Timezone                                                                                 |
|version                 |Integer          |The Dag version. Can be incremented if logic changes                                     |
|sla_mins                |Integer          |SLA mins                                                                                 |

### Table Configuration Parameters

|Name                    |Data Type        |Description                                                                              |
|------------------------|-----------------|-----------------------------------------------------------------------------------------|
|table_name                      |String           |Target table name                                              |
|landing_zone_table_name_override|String (Optional)|Optional staging zone table name                               |
|dest_table_override             |String (Optional)|Optional target table name. If None, table_name is used instead|
|source_table_schema_object      |String (Optional)|Google Cloud Storage schema object URI                         |
|surrogate_keys                  |List[String]     |Keys used to identify unique records when merging into ODS     |
|column_mapping                  |Dictionary       |Mapping used to rename columns                                 |
|ods_config                      |OdsTableConfig   |ODS table configuration options                                |
|hds_config                      |HdsTableConfig   |HDS table configuration options                                |
|version                         |Integer          |The Dag version. Can be incremented if logic changes           |

### ODS Configuration Parameters

|Name                    |Data Type        |Description                                                                              |
|------------------------|-----------------|-----------------------------------------------------------------------------------------|
|ingestion_type                  |IngestionType    |FULL or INCREMENTAL                                            |
|ingestion_time_column_name      |String           |Insert time column                                             |
|update_time_column_name         |String           |Update time column                                             |
|primary_key_hash_column_name    |String           |Primary key hash column                                        |
|hash_column_name                |String           |Row hash column                                                |

### HDS Configuration Parameters

|Name                    |Data Type        |Description                                                                              |
|------------------------|-----------------|-----------------------------------------------------------------------------------------|
|hds_table_type                  |HdsTableType     |SNAPSHOT or SCD2                                               |
|hds_table_time_partitioning     |TimePartitioning |Partitioning for BigQuery table. One of DAY, HOUR, or MONTH    |
|eff_start_time_column_name      |String           |Insert time column                                             |
|eff_end_time_column_name        |String           |Expiry time column                                             |
|hash_column_name                |String           |Row hash column                                                |
|partition_time_column_name      |String           |Partition time column                                          |.
